Please run: hw3_1.py

1. Does your network reach 0 training error? 
A: YES


2. Can you make your program into stochastic gradient descent (SGD)?
A: YES

3. Does SGD give lower test error than full gradient descent?
A: Yes but runs slower than full gradient descent

4. What happens if change the activation to sign? Will the same algorithm
work? If not what will you change to make the algorithm converge to a local
minimum?
A: sign is not differentiable so it does not work.
We can use any activation function that is differentiable. 
Non linearity is achieved only by activating by a function that is differentiable across all points.

